{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Load data set & Get idea**"
      ],
      "metadata": {
        "id": "AK8odXy-RRou"
      },
      "id": "AK8odXy-RRou"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff76077a-7b8b-4945-b675-5eeb8bc59bf6",
      "metadata": {
        "id": "ff76077a-7b8b-4945-b675-5eeb8bc59bf6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/vihanga-induwara/CM2604-CW-Bank-Marketing/refs/heads/main/bank%2Bmarketing/bank-additional/bank-additional/bank-additional-full.csv\", sep=\";\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "Ir96EaSmYorx"
      },
      "id": "Ir96EaSmYorx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check column names and data types\n",
        "print(data.info())"
      ],
      "metadata": {
        "id": "4oBXyK8pZQ0O"
      },
      "id": "4oBXyK8pZQ0O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(data.isnull().sum())"
      ],
      "metadata": {
        "id": "80Kzfqa0ZRae"
      },
      "id": "80Kzfqa0ZRae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary statistics\n",
        "print(data.describe())"
      ],
      "metadata": {
        "id": "CTeNslycZRrG"
      },
      "id": "CTeNslycZRrG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Age distribution\n",
        "sns.histplot(data['age'], bins=20, kde=True, color='blue')\n",
        "plt.title('Age Distribution')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sGAf0l6baKpL"
      },
      "id": "sGAf0l6baKpL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Countplot for target variable\n",
        "sns.countplot(x='y', data=data, palette='pastel')\n",
        "plt.title('Distribution of Campaign Outcome (Balanced vs. Imbalanced)')\n",
        "plt.xlabel('Outcome')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ammVD5TmaYzU"
      },
      "id": "ammVD5TmaYzU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate percentage distribution\n",
        "outcome_counts = data['y'].value_counts(normalize=True) * 100\n",
        "print(outcome_counts)\n",
        "\n",
        "# Visualize as a pie chart\n",
        "outcome_counts.plot.pie(autopct='%1.1f%%', labels=['No', 'Yes'], colors=['lightblue', 'orange'])\n",
        "plt.title('Percentage Distribution of Campaign Outcome')\n",
        "plt.ylabel('')  # Remove y-axis label\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "O-b9-n3gbAE1"
      },
      "id": "O-b9-n3gbAE1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocess Data**"
      ],
      "metadata": {
        "id": "MzquUa1FUL4p"
      },
      "id": "MzquUa1FUL4p"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **idea of data**"
      ],
      "metadata": {
        "id": "B49tFbde2jnK"
      },
      "id": "B49tFbde2jnK"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Display column details: type and number of unique categories\n",
        "for column in data.columns:\n",
        "    print(f\"Column: {column}\")\n",
        "    print(f\"Type: {data[column].dtype}\")\n",
        "    print(f\"Number of unique values: {data[column].nunique()}\")\n",
        "\n",
        "    # If the column is categorical, print the unique values (categories)\n",
        "    if data[column].dtype == 'object':\n",
        "        print(f\"Categories: {data[column].unique()}\")\n",
        "    print(\"-\" * 40)\n"
      ],
      "metadata": {
        "id": "NoXOg4zAidLN"
      },
      "id": "NoXOg4zAidLN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **age**"
      ],
      "metadata": {
        "id": "XjlOHsa1tER3"
      },
      "id": "XjlOHsa1tER3"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data[\"age\"].value_counts()"
      ],
      "metadata": {
        "id": "hNGjtmwYQomK"
      },
      "id": "hNGjtmwYQomK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(data[\"age\"], bins=30, color='skyblue', edgecolor='black')\n",
        "plt.title(\"Distribution of Age\", fontsize=16)\n",
        "plt.xlabel(\"Age\", fontsize=12)\n",
        "plt.ylabel(\"Frequency\", fontsize=12)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ABdl_pjGSszc"
      },
      "id": "ABdl_pjGSszc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Calculate Q1 (25th percentile), Q3 (75th percentile), and IQR\n",
        "Q1 = data['age'].quantile(0.25)\n",
        "Q3 = data['age'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define the lower and upper bounds\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "print(f\"Lower Bound: {lower_bound}, Upper Bound: {upper_bound}\")\n",
        "\n",
        "# Filter the data to exclude outliers\n",
        "data = data[(data['age'] >= lower_bound) & (data['age'] <= upper_bound)]\n",
        "\n",
        "# Print the shape of the dataset before and after removing outliers\n",
        "print(f\"Original data shape: {data.shape}\")\n",
        "print(f\"Data shape after removing outliers: {data.shape}\")\n"
      ],
      "metadata": {
        "id": "NwaD3HhqUMY2"
      },
      "id": "NwaD3HhqUMY2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(data[\"age\"], bins=30, color='skyblue', edgecolor='black')\n",
        "plt.title(\"Distribution of Age\", fontsize=16)\n",
        "plt.xlabel(\"Age\", fontsize=12)\n",
        "plt.ylabel(\"Frequency\", fontsize=12)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bkDWiI5tURyt"
      },
      "id": "bkDWiI5tURyt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Create a scaler instance\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Normalize the 'age' column\n",
        "data['age'] = scaler.fit_transform(data[['age']])\n",
        "\n",
        "# Display the first few rows of the normalized column\n",
        "print(data[['age']].head())\n"
      ],
      "metadata": {
        "id": "FQm2MwCPYswE"
      },
      "id": "FQm2MwCPYswE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(data[\"age\"], bins=30, color='skyblue', edgecolor='black')\n",
        "plt.title(\"Distribution of Age\", fontsize=16)\n",
        "plt.xlabel(\"Age\", fontsize=12)\n",
        "plt.ylabel(\"Frequency\", fontsize=12)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0_h6V4IaY7m5"
      },
      "id": "0_h6V4IaY7m5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **job**"
      ],
      "metadata": {
        "id": "jaQ4JuV1tMk2"
      },
      "id": "jaQ4JuV1tMk2"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data[\"job\"].value_counts()"
      ],
      "metadata": {
        "id": "vHPzn99ZQp42"
      },
      "id": "vHPzn99ZQp42",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming you have a DataFrame called 'data'\n",
        "data_encoded = pd.get_dummies(data['job'], prefix='job', drop_first=False)\n",
        "\n",
        "# If you want to concatenate it with the original dataframe\n",
        "data = pd.concat([data, data_encoded], axis=1)\n",
        "\n",
        "# Display the first few rows of the updated dataframe\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "nyJsGJ2bZS1F"
      },
      "id": "nyJsGJ2bZS1F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 'job' column in-place\n",
        "data.drop(columns=['job'], inplace=True)\n",
        "\n",
        "# Verify if the column is removed\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "hZDNnf774zot"
      },
      "id": "hZDNnf774zot",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **marital**"
      ],
      "metadata": {
        "id": "B9xIWeqdtM3T"
      },
      "id": "B9xIWeqdtM3T"
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"marital\"].value_counts()"
      ],
      "metadata": {
        "id": "058STPsgQq2F"
      },
      "id": "058STPsgQq2F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace unknown with the most frequent category\n",
        "most_frequent_marital = data['marital'].mode()[0]\n",
        "data['marital'].replace('unknown', most_frequent_marital, inplace=True)\n"
      ],
      "metadata": {
        "id": "OiecgPuIZ1uJ"
      },
      "id": "OiecgPuIZ1uJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"marital\"].value_counts()"
      ],
      "metadata": {
        "id": "7kfsZMoYaqx9"
      },
      "id": "7kfsZMoYaqx9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode the 'marital' column\n",
        "data = pd.get_dummies(data, columns=['marital'], drop_first=False)\n",
        "\n",
        "# Check the result\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "c6RCK1ZYa9t2"
      },
      "id": "c6RCK1ZYa9t2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **education**"
      ],
      "metadata": {
        "id": "oeqGS5RetNGK"
      },
      "id": "oeqGS5RetNGK"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data[\"education\"].value_counts()"
      ],
      "metadata": {
        "id": "5fYGrRZuQsC7"
      },
      "id": "5fYGrRZuQsC7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the most frequent category in the 'education' column\n",
        "most_frequent_education = data['education'].mode()[0]\n",
        "\n",
        "# Replace 'illiterate' with the most frequent category\n",
        "data['education'] = data['education'].replace('illiterate', most_frequent_education)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k4_ZoudhZpl0"
      },
      "id": "k4_ZoudhZpl0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the value counts after replacement\n",
        "print(data['education'].value_counts())"
      ],
      "metadata": {
        "id": "yQL50iQKb10_"
      },
      "id": "yQL50iQKb10_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode the 'education' column\n",
        "education_encoded = pd.get_dummies(data['education'], prefix='education')\n",
        "\n",
        "# Join the one-hot encoded columns back to the original DataFrame\n",
        "data = pd.concat([data, education_encoded], axis=1)\n",
        "\n",
        "# Drop the original 'education' column\n",
        "data.drop('education', axis=1, inplace=True)\n",
        "\n",
        "# Print the updated DataFrame\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "N5gjHOxuvSYF"
      },
      "id": "N5gjHOxuvSYF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **default**"
      ],
      "metadata": {
        "id": "6jhCoi0EtNVh"
      },
      "id": "6jhCoi0EtNVh"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data[\"default\"].value_counts()"
      ],
      "metadata": {
        "id": "i0TGikmZQtM2"
      },
      "id": "i0TGikmZQtM2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 'default' column if it doesn't provide useful information\n",
        "data = data.drop('default', axis=1)\n",
        "\n",
        "# Check the remaining columns\n",
        "print(data.columns)\n"
      ],
      "metadata": {
        "id": "ngCSOClycpE9"
      },
      "id": "ngCSOClycpE9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **housing**"
      ],
      "metadata": {
        "id": "ndAcQb_EtNjp"
      },
      "id": "ndAcQb_EtNjp"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data[\"housing\"].value_counts()"
      ],
      "metadata": {
        "id": "CHqchYbvQt_I"
      },
      "id": "CHqchYbvQt_I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace 'unknown' with the most frequent value ('yes')\n",
        "data['housing'] = data['housing'].replace('unknown', 'yes')\n",
        "\n",
        "# Check the updated value counts\n",
        "print(data['housing'].value_counts())\n"
      ],
      "metadata": {
        "id": "tNuf-HlCdV3r"
      },
      "id": "tNuf-HlCdV3r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode the 'housing' column\n",
        "housing_encoded = pd.get_dummies(data['housing'], prefix='housing')\n",
        "\n",
        "# Join the encoded columns back to the original dataframe\n",
        "data = pd.concat([data, housing_encoded], axis=1)\n",
        "\n",
        "# Drop the original 'housing' column if it's no longer needed\n",
        "data = data.drop(columns=['housing'])\n",
        "\n",
        "# Check the updated dataframe\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "1sm0sSNjdZmk"
      },
      "id": "1sm0sSNjdZmk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **loan**"
      ],
      "metadata": {
        "id": "bZNoOjEhtNyK"
      },
      "id": "bZNoOjEhtNyK"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data[\"loan\"].value_counts()"
      ],
      "metadata": {
        "id": "zP8fSOZcQvCK"
      },
      "id": "zP8fSOZcQvCK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace 'unknown' with the most frequent value ('yes')\n",
        "data['loan'] = data['loan'].replace('unknown', 'yes')\n",
        "\n",
        "# Check the updated value counts\n",
        "print(data['loan'].value_counts())\n"
      ],
      "metadata": {
        "id": "zj7FvrVqdvMf"
      },
      "id": "zj7FvrVqdvMf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode the 'loan' column\n",
        "loan_encoded = pd.get_dummies(data['loan'], prefix='loan')\n",
        "\n",
        "# Join the encoded columns back to the original dataframe\n",
        "data = pd.concat([data, loan_encoded], axis=1)\n",
        "\n",
        "# Drop the original 'loan' column if it's no longer needed\n",
        "data = data.drop(columns=['loan'])\n",
        "\n",
        "# Check the updated dataframe\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "jKnommj4eCto"
      },
      "id": "jKnommj4eCto",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **contact**"
      ],
      "metadata": {
        "id": "GhEGYwg2tN_0"
      },
      "id": "GhEGYwg2tN_0"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data[\"contact\"].value_counts()"
      ],
      "metadata": {
        "id": "d2q-NZoWQv_a"
      },
      "id": "d2q-NZoWQv_a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 'contact' column\n",
        "data = data.drop(columns=['contact'])\n",
        "\n",
        "# Check the updated dataframe\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "H1LMJzKreWti"
      },
      "id": "H1LMJzKreWti",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **month**"
      ],
      "metadata": {
        "id": "2SEbsCAFtONy"
      },
      "id": "2SEbsCAFtONy"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data[\"month\"].value_counts()"
      ],
      "metadata": {
        "id": "NxYQbubPQ6BO"
      },
      "id": "NxYQbubPQ6BO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the value counts of the 'month' column\n",
        "data['month'].value_counts().plot(kind='bar', color='skyblue')\n",
        "\n",
        "# Set labels and title\n",
        "plt.title('Distribution of Month Values')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Show the plot\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vcp80AaRe2ym"
      },
      "id": "vcp80AaRe2ym",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'data' is your DataFrame\n",
        "data = pd.get_dummies(data, columns=['month'], drop_first=False)\n",
        "\n",
        "# Display the transformed data to check the result\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "dnRMoP0Bfgz0"
      },
      "id": "dnRMoP0Bfgz0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **day_of_week**"
      ],
      "metadata": {
        "id": "drybQXXxtOea"
      },
      "id": "drybQXXxtOea"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data[\"day_of_week\"].value_counts()"
      ],
      "metadata": {
        "id": "JzYN5_fMQ5VR"
      },
      "id": "JzYN5_fMQ5VR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the value counts of the 'day_of_week' column\n",
        "data['day_of_week'].value_counts().plot(kind='bar', color='skyblue')\n",
        "\n",
        "# Set labels and title\n",
        "plt.title('Distribution of Days')\n",
        "plt.xlabel('Day')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Show the plot\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xo9BgHcBfwsC"
      },
      "id": "xo9BgHcBfwsC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 'day_of_week' column in-place\n",
        "data.drop(columns=['day_of_week'], inplace=True)\n",
        "\n",
        "# Verify if the column is removed\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "EDroX9AEgIEL"
      },
      "id": "EDroX9AEgIEL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **duration**"
      ],
      "metadata": {
        "id": "geqiSWLFtOtB"
      },
      "id": "geqiSWLFtOtB"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data[\"duration\"].value_counts()"
      ],
      "metadata": {
        "id": "O7OkIReJQ4qy"
      },
      "id": "O7OkIReJQ4qy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the distribution of the 'duration' column\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(data['duration'], bins=30, color='skyblue', edgecolor='black')\n",
        "plt.title('Distribution of Duration')\n",
        "plt.xlabel('Duration')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Z8zqXZaLhXW1"
      },
      "id": "Z8zqXZaLhXW1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Reshape the data as it is a single column\n",
        "scaler = MinMaxScaler()\n",
        "data['duration'] = scaler.fit_transform(data[['duration']])\n",
        "\n",
        "# Check the result\n",
        "print(data[['duration']].head())\n"
      ],
      "metadata": {
        "id": "XdOSkmYMhuQw"
      },
      "id": "XdOSkmYMhuQw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **campaign**"
      ],
      "metadata": {
        "id": "3QNlm1sbtO7W"
      },
      "id": "3QNlm1sbtO7W"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data[\"campaign\"].value_counts()"
      ],
      "metadata": {
        "id": "OdJ8KosDQ3ie"
      },
      "id": "OdJ8KosDQ3ie",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Reshape the data as it is a single column\n",
        "scaler = MinMaxScaler()\n",
        "data['campaign'] = scaler.fit_transform(data[['campaign']])\n",
        "\n",
        "# Check the result\n",
        "print(data[['campaign']].head())\n"
      ],
      "metadata": {
        "id": "duGjsTrLieug"
      },
      "id": "duGjsTrLieug",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data[\"campaign\"].value_counts()"
      ],
      "metadata": {
        "id": "DOAuFHFMpstQ"
      },
      "id": "DOAuFHFMpstQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **pdays**"
      ],
      "metadata": {
        "id": "sbeuhMU5tPLM"
      },
      "id": "sbeuhMU5tPLM"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data[\"pdays\"].value_counts()"
      ],
      "metadata": {
        "id": "VpABvvV6Q2kK"
      },
      "id": "VpABvvV6Q2kK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the value counts of the 'pdays' column\n",
        "pdays_counts = data[\"pdays\"].value_counts()\n",
        "\n",
        "# Plot the result\n",
        "pdays_counts.plot(kind='bar', figsize=(10,6), color='skyblue')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('pdays Values')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Value Counts of pdays')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RXrpqYyIkJzd"
      },
      "id": "RXrpqYyIkJzd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 'pdays' column\n",
        "data = data.drop(columns=['pdays'])\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "cZg5JUIHkN1t"
      },
      "id": "cZg5JUIHkN1t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **previous**"
      ],
      "metadata": {
        "id": "0LogaybPtPYd"
      },
      "id": "0LogaybPtPYd"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data[\"previous\"].value_counts()"
      ],
      "metadata": {
        "id": "Q3Cq0TqUQ197"
      },
      "id": "Q3Cq0TqUQ197",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the value counts for the 'previous' column\n",
        "data['previous'].value_counts().sort_index().plot(kind='bar')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Previous')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Previous Column')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rI0AFtnSk4UZ"
      },
      "id": "rI0AFtnSk4UZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 'pdays' column\n",
        "data = data.drop(columns=['previous'])\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "bHWY_FrCk_SQ"
      },
      "id": "bHWY_FrCk_SQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **poutcome**"
      ],
      "metadata": {
        "id": "kTPg3vl1tPlB"
      },
      "id": "kTPg3vl1tPlB"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data[\"poutcome\"].value_counts()"
      ],
      "metadata": {
        "id": "n324Tz3jQ1SY"
      },
      "id": "n324Tz3jQ1SY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 'pdays' column\n",
        "data = data.drop(columns=['poutcome'])\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "Wbxm4n3Vl3pD"
      },
      "id": "Wbxm4n3Vl3pD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **emp.var.rate**"
      ],
      "metadata": {
        "id": "jR4ftpdPtPyv"
      },
      "id": "jR4ftpdPtPyv"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data[\"emp.var.rate\"].value_counts()"
      ],
      "metadata": {
        "id": "KZld2TpMQ0Lp"
      },
      "id": "KZld2TpMQ0Lp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(data['emp.var.rate'], kde=True, bins=30)  # kde=True adds a Kernel Density Estimate\n",
        "plt.title('Histogram of Employment Variation Rate with Outliers')\n",
        "plt.xlabel('Employment Variation Rate')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5J90aDRsnkZs"
      },
      "id": "5J90aDRsnkZs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Normalize the 'emp.var.rate' column using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "data['emp.var.rate'] = scaler.fit_transform(data[['emp.var.rate']])\n"
      ],
      "metadata": {
        "id": "uK2WZQVxmXry"
      },
      "id": "uK2WZQVxmXry",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data[\"emp.var.rate\"].value_counts()"
      ],
      "metadata": {
        "id": "A7GWo9wzmcoz"
      },
      "id": "A7GWo9wzmcoz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(data['emp.var.rate'], kde=True, bins=30)  # kde=True adds a Kernel Density Estimate\n",
        "plt.title('Histogram of Employment Variation Rate with Outliers')\n",
        "plt.xlabel('Employment Variation Rate')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6XBZAw4Jm8gl"
      },
      "id": "6XBZAw4Jm8gl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **cons.price.idx**"
      ],
      "metadata": {
        "id": "KE8DKYmutQBa"
      },
      "id": "KE8DKYmutQBa"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data[\"cons.price.idx\"].value_counts()"
      ],
      "metadata": {
        "id": "mqDmFgAJQzmC"
      },
      "id": "mqDmFgAJQzmC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.histplot(data['cons.price.idx'], kde=True, bins=30)\n",
        "plt.title('Distribution of cons.price.idx')\n",
        "plt.xlabel('cons.price.idx')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Gttl8-pnq9Jt"
      },
      "id": "Gttl8-pnq9Jt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q1 = data['cons.price.idx'].quantile(0.25)\n",
        "Q3 = data['cons.price.idx'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Identify outliers\n",
        "outliers = data[(data['cons.price.idx'] < (Q1 - 1.5 * IQR)) | (data['cons.price.idx'] > (Q3 + 1.5 * IQR))]\n",
        "\n",
        "# Print the outliers\n",
        "print(outliers)\n"
      ],
      "metadata": {
        "id": "9MWVPgY6rU1F"
      },
      "id": "9MWVPgY6rU1F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove outliers\n",
        "data_no_outliers = data[(data['cons.price.idx'] >= (Q1 - 1.5 * IQR)) & (data['cons.price.idx'] <= (Q3 + 1.5 * IQR))]\n",
        "\n",
        "# Verify the data without outliers\n",
        "print(data_no_outliers.head())\n"
      ],
      "metadata": {
        "id": "NdzhbfQBreBl"
      },
      "id": "NdzhbfQBreBl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Value counts for cons.price.idx\n",
        "print(data['cons.price.idx'].value_counts())\n"
      ],
      "metadata": {
        "id": "7dk7aPF4rj0D"
      },
      "id": "7dk7aPF4rj0D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Reshape the data (as it should be a 2D array for scaling)\n",
        "data['cons.price.idx'] = scaler.fit_transform(data[['cons.price.idx']])\n",
        "\n",
        "# Check the normalized values\n",
        "print(data[['cons.price.idx']].head())\n"
      ],
      "metadata": {
        "id": "yrZ81QkorvcJ"
      },
      "id": "yrZ81QkorvcJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **cons.conf.idx**"
      ],
      "metadata": {
        "id": "JS22y4rLtQPY"
      },
      "id": "JS22y4rLtQPY"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data[\"cons.conf.idx\"].value_counts()"
      ],
      "metadata": {
        "id": "UJVpKduPQy1c"
      },
      "id": "UJVpKduPQy1c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Reshape the data (as it should be a 2D array for scaling)\n",
        "data['cons.conf.idx_normalized'] = scaler.fit_transform(data[['cons.conf.idx']])\n",
        "\n",
        "# Check the normalized values\n",
        "print(data[['cons.conf.idx', 'cons.conf.idx_normalized']].head())\n"
      ],
      "metadata": {
        "id": "AkwqY3AMsTQB"
      },
      "id": "AkwqY3AMsTQB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 'cons.conf.idx' column\n",
        "data = data.drop(columns=['cons.conf.idx'])\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "eHtEd9_MsX6L"
      },
      "id": "eHtEd9_MsX6L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **euribor3m**"
      ],
      "metadata": {
        "id": "_XRKTvivtQeh"
      },
      "id": "_XRKTvivtQeh"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data[\"euribor3m\"].value_counts()"
      ],
      "metadata": {
        "id": "kEEcn3ApQyO7"
      },
      "id": "kEEcn3ApQyO7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Reshape the data (since MinMaxScaler expects 2D data)\n",
        "data['euribor3m_normalized'] = scaler.fit_transform(data[['euribor3m']])\n",
        "\n",
        "# Check the normalized values\n",
        "print(data[['euribor3m', 'euribor3m_normalized']].head())\n"
      ],
      "metadata": {
        "id": "q8SQqHuns2AQ"
      },
      "id": "q8SQqHuns2AQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 'euribor3m' column\n",
        "data = data.drop(columns=['euribor3m'])\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "-o9Kys8ItI1H"
      },
      "id": "-o9Kys8ItI1H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **nr.employed**"
      ],
      "metadata": {
        "id": "OHZAXLtguISd"
      },
      "id": "OHZAXLtguISd"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data[\"nr.employed\"].value_counts()"
      ],
      "metadata": {
        "id": "6by_DflYQxkC"
      },
      "id": "6by_DflYQxkC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Reshape the data (since MinMaxScaler expects 2D data)\n",
        "data['nr.employed_normalized'] = scaler.fit_transform(data[['nr.employed']])\n",
        "\n",
        "# Check the normalized values\n",
        "print(data[['nr.employed', 'nr.employed_normalized']].head())\n"
      ],
      "metadata": {
        "id": "Huj0xvcatp4D"
      },
      "id": "Huj0xvcatp4D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 'nr.employed' column\n",
        "data = data.drop(columns=['nr.employed'])\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "CVXVNzLwtuPW"
      },
      "id": "CVXVNzLwtuPW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Y**"
      ],
      "metadata": {
        "id": "0dgU_GLH7FgL"
      },
      "id": "0dgU_GLH7FgL"
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the 'y' column\n",
        "data['y'] = data['y'].map({'yes': 1, 'no': 0})\n",
        "\n",
        "# Verify the encoding\n",
        "print(data['y'].value_counts())\n"
      ],
      "metadata": {
        "id": "KGJNZ73r7EcS"
      },
      "id": "KGJNZ73r7EcS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **After Colum wised prepoesed**"
      ],
      "metadata": {
        "id": "NY9D26gV4EY8"
      },
      "id": "NY9D26gV4EY8"
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "YTZN_9HDt0_p"
      },
      "id": "YTZN_9HDt0_p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display all columns in the DataFrame\n",
        "pd.set_option('display.max_columns', None)  # Show all columns\n",
        "print(data.head())  # Display the first few rows"
      ],
      "metadata": {
        "id": "x04wMmkV6KH_"
      },
      "id": "x04wMmkV6KH_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **handle data set imbalance**"
      ],
      "metadata": {
        "id": "fSaOicg37zDp"
      },
      "id": "fSaOicg37zDp"
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "# Split data into features and target\n",
        "X = data.drop(columns=['y'])\n",
        "y = data['y']\n",
        "\n",
        "# Split into training and testing sets with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Check class distribution before resampling\n",
        "print(\"Before SMOTE:\", Counter(y_train))\n",
        "\n",
        "# Apply SMOTE to training data only\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Check class distribution after SMOTE\n",
        "print(\"After SMOTE:\", Counter(y_train_resampled))\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_resampled = scaler.fit_transform(X_train_resampled)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Combine the resampled data into a single DataFrame (optional)\n",
        "balanced_data = pd.DataFrame(X_train_resampled, columns=X.columns)  # Use original column names\n",
        "balanced_data['y'] = y_train_resampled  # Add the target label column\n",
        "\n",
        "# Now you can train your model using X_train_resampled and y_train_resampled\n"
      ],
      "metadata": {
        "id": "4mAiCjTO74Y4"
      },
      "id": "4mAiCjTO74Y4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# Count class distribution before and after SMOTE\n",
        "before_counts = Counter(y_train)  # Original training data (imbalanced)\n",
        "after_counts = Counter(y_train_resampled)  # Resampled training data (balanced)\n",
        "\n",
        "# Create a DataFrame for visualization\n",
        "df = pd.DataFrame({\n",
        "    'Class': ['No', 'Yes'],\n",
        "    'Before SMOTE': [before_counts[0], before_counts[1]],\n",
        "    'After SMOTE': [after_counts[0], after_counts[1]]\n",
        "})\n",
        "\n",
        "# Plot the class distribution\n",
        "df.set_index('Class').plot(kind='bar', figsize=(8, 5), color=['skyblue', 'orange'])\n",
        "plt.title('Class Distribution Before and After SMOTE')\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Class')\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.legend(title='Dataset')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "77KxQIUN9BI0"
      },
      "id": "77KxQIUN9BI0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display all columns in the DataFrame\n",
        "pd.set_option('display.max_columns', None)  # Show all columns\n",
        "print(data.head())  # Display the first few rows"
      ],
      "metadata": {
        "id": "VjRHSk7i9tRR"
      },
      "id": "VjRHSk7i9tRR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature engineering**"
      ],
      "metadata": {
        "id": "PlMFav0CUZMu"
      },
      "id": "PlMFav0CUZMu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature Extraction**"
      ],
      "metadata": {
        "id": "J59l62QRfUSC"
      },
      "id": "J59l62QRfUSC"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "\n",
        "# Define features for PCA\n",
        "features = ['emp.var.rate', 'cons.price.idx','cons.conf.idx_normalized', 'euribor3m_normalized', 'nr.employed_normalized']\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA(n_components=1)\n",
        "principal_components = pca.fit_transform(data[features])\n",
        "\n",
        "# Add PC1 to the original dataset\n",
        "data['PC1'] = principal_components\n",
        "\n",
        "# Drop the original columns used for PCA\n",
        "data = data.drop(columns=features)\n",
        "\n",
        "# Print the explained variance ratio for PCA\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(\"Explained Variance Ratio:\", explained_variance)\n"
      ],
      "metadata": {
        "id": "anEUUFN3g7ks"
      },
      "id": "anEUUFN3g7ks",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "kOkUDrCRj8K0"
      },
      "id": "kOkUDrCRj8K0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature Selection**"
      ],
      "metadata": {
        "id": "FhJz_lGEfSRk"
      },
      "id": "FhJz_lGEfSRk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Random forest model**"
      ],
      "metadata": {
        "id": "lxCcQ7Avr1Oy"
      },
      "id": "lxCcQ7Avr1Oy"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import pandas as pd\n",
        "\n",
        "# Get categorical and numerical columns (modified to handle no categorical columns)\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "numerical_cols = X.select_dtypes(exclude=['object']).columns\n",
        "\n",
        "# Preprocess categorical features with OneHotEncoder (only if there are categorical columns)\n",
        "if len(categorical_cols) > 0:\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', 'passthrough', numerical_cols),\n",
        "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Apply the preprocessor and fit the transformer\n",
        "    X_encoded = preprocessor.fit_transform(X)\n",
        "\n",
        "    # Get the names of the encoded columns\n",
        "    encoded_columns = preprocessor.transformers_[1][1].get_feature_names_out(categorical_cols)\n",
        "\n",
        "    # Combine numerical and encoded feature names\n",
        "    all_columns = list(numerical_cols) + list(encoded_columns)\n",
        "else:\n",
        "    # If no categorical columns, use numerical columns directly\n",
        "    X_encoded = X[numerical_cols].values  # Convert to NumPy array\n",
        "    all_columns = list(numerical_cols)\n",
        "\n",
        "# Fit a random forest model\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_encoded, y)\n",
        "\n",
        "# Get feature importance\n",
        "feature_importance = pd.Series(rf.feature_importances_, index=all_columns)\n",
        "\n",
        "# Display features with low importance\n",
        "weak_features_rf = feature_importance[feature_importance < 0.01]\n",
        "print(\"Features with low importance (Random Forest):\")\n",
        "print(weak_features_rf)"
      ],
      "metadata": {
        "id": "W37tn6L8fhQs"
      },
      "id": "W37tn6L8fhQs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Lasso model**"
      ],
      "metadata": {
        "id": "70OqD6aBrt40"
      },
      "id": "70OqD6aBrt40"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Fit a Lasso model\n",
        "lasso = Lasso(alpha=0.01)  # Adjust alpha as necessary\n",
        "lasso.fit(X_scaled, y)\n",
        "\n",
        "# Get the coefficients of the model\n",
        "coef = pd.Series(lasso.coef_, index=X.columns)\n",
        "\n",
        "# Show features with zero or near-zero coefficients (less important)\n",
        "weak_features_lasso = coef[coef == 0]\n",
        "print(\"Features with near-zero coefficients (less important):\")\n",
        "print(weak_features_lasso)\n"
      ],
      "metadata": {
        "id": "iGqyaR_LlD1g"
      },
      "id": "iGqyaR_LlD1g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **correlation**"
      ],
      "metadata": {
        "id": "Dq0YNfNPrlpH"
      },
      "id": "Dq0YNfNPrlpH"
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate correlation between each feature and target variable `y`\n",
        "correlation_matrix = data.corr()\n",
        "\n",
        "# Find correlation with 'y'\n",
        "correlation_with_y = correlation_matrix['y'].sort_values(ascending=False)\n",
        "\n",
        "# Display features with weak correlation (e.g., correlation less than a threshold)\n",
        "weak_correlations = correlation_with_y[abs(correlation_with_y) < 0.1]\n",
        "print(\"Features with weak correlation to y:\")\n",
        "print(weak_correlations)\n"
      ],
      "metadata": {
        "id": "IrENd0fRk1c0"
      },
      "id": "IrENd0fRk1c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ANOVA F-test**"
      ],
      "metadata": {
        "id": "IuYRfq9isDIc"
      },
      "id": "IuYRfq9isDIc"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import f_classif\n",
        "\n",
        "X = data.drop(columns=['y'])\n",
        "y = data['y']\n",
        "\n",
        "# Perform ANOVA F-test\n",
        "F_values, p_values = f_classif(X, y)\n",
        "\n",
        "# Create a DataFrame to show the F-values and p-values for each feature\n",
        "feature_scores = pd.DataFrame({'Feature': X.columns, 'F-value': F_values, 'p-value': p_values})\n",
        "\n",
        "# Features with high p-value (>0.05) indicate less significance\n",
        "weak_features = feature_scores[feature_scores['p-value'] > 0.05]\n",
        "print(\"Features with weak effect on y (p-value > 0.05):\")\n",
        "print(weak_features)\n"
      ],
      "metadata": {
        "id": "7hqGxLxHk7m_"
      },
      "id": "7hqGxLxHk7m_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Output**"
      ],
      "metadata": {
        "id": "JLE4jWUlsI2A"
      },
      "id": "JLE4jWUlsI2A"
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "3MYyPeKvlo_q"
      },
      "id": "3MYyPeKvlo_q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Combine and drop**"
      ],
      "metadata": {
        "id": "fbkxTNl4stf9"
      },
      "id": "fbkxTNl4stf9"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import f_classif\n",
        "\n",
        "\n",
        "# 1. Random Forest Feature Importance\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X, y)\n",
        "feature_importance = pd.Series(rf.feature_importances_, index=X.columns)\n",
        "weak_features_rf = feature_importance[feature_importance < 0.01].index\n",
        "\n",
        "# 2. Lasso Model (near-zero coefficients)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "lasso = Lasso(alpha=0.01)\n",
        "lasso.fit(X_scaled, y)\n",
        "lasso_coef = pd.Series(lasso.coef_, index=X.columns)\n",
        "weak_features_lasso = lasso_coef[lasso_coef == 0].index\n",
        "\n",
        "# 3. Correlation with target variable y (weak correlation)\n",
        "correlation_matrix = X.corrwith(y)\n",
        "weak_correlations = correlation_matrix[abs(correlation_matrix) < 0.1].index\n",
        "\n",
        "# 4. ANOVA F-test (high p-value indicates weak effect)\n",
        "F_values, p_values = f_classif(X, y)\n",
        "feature_scores = pd.DataFrame({'Feature': X.columns, 'F-value': F_values, 'p-value': p_values})\n",
        "weak_features_anova = feature_scores[feature_scores['p-value'] > 0.05]['Feature']\n",
        "\n",
        "# Combine the outputs into a single DataFrame, ensuring only the feature names (index) are included\n",
        "summary_df = pd.DataFrame({\n",
        "    'Random_Forest_Importance': pd.Series(list(weak_features_rf), index=weak_features_rf),\n",
        "    'Lasso_Coefficients': pd.Series(list(weak_features_lasso), index=weak_features_lasso),\n",
        "    'Correlation_with_y': pd.Series(list(weak_correlations), index=weak_correlations),\n",
        "    'ANOVA_p_value': pd.Series(list(weak_features_anova), index=weak_features_anova)\n",
        "})\n",
        "\n",
        "# Set a threshold for features being weak (if they appear weak in multiple methods)\n",
        "threshold = 2  # Number of methods in which a feature is considered weak\n",
        "\n",
        "# Count how many times each feature appears in the \"weak\" category\n",
        "weak_features_count = (summary_df.notna()).sum(axis=1)\n",
        "\n",
        "# Features to remove: those that are weak in at least `threshold` methods\n",
        "features_to_remove = weak_features_count[weak_features_count >= threshold].index\n",
        "\n",
        "# Print features that have weak effects on y\n",
        "print(\"Features with weak effect across multiple methods:\")\n",
        "print(features_to_remove)\n",
        "\n",
        "# Now, remove the least important features\n",
        "X_reduced = X.drop(columns=features_to_remove)\n",
        "\n",
        "# Optional: If you want to remove exactly the top N weak features, you can sort and select the top N:\n",
        "top_n_to_remove = 5\n",
        "sorted_weak_features = weak_features_count.sort_values(ascending=False).head(top_n_to_remove).index\n",
        "X_reduced = X.drop(columns=sorted_weak_features)\n",
        "\n",
        "# Print the final set of features to be removed\n",
        "print(f\"Top {top_n_to_remove} features to remove based on combined methods:\")\n",
        "print(sorted_weak_features)\n"
      ],
      "metadata": {
        "id": "we9FlEaGs02t"
      },
      "id": "we9FlEaGs02t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the columns listed in sorted_weak_features from the 'data' DataFrame\n",
        "data_reduced = data.drop(columns=sorted_weak_features)\n",
        "\n",
        "# Print the resulting DataFrame after removal\n",
        "print(\"Data after dropping the weak features:\")\n",
        "print(data_reduced.head())  # Display the first few rows of the reduced dataset\n",
        "\n",
        "# Optionally, assign the reduced DataFrame back to the original variable `data`\n",
        "data = data_reduced"
      ],
      "metadata": {
        "id": "YB7tVDkbuAyV"
      },
      "id": "YB7tVDkbuAyV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "tslBILOtv_DU"
      },
      "id": "tslBILOtv_DU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train Model**"
      ],
      "metadata": {
        "id": "6V_wBxn1RnDF"
      },
      "id": "6V_wBxn1RnDF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import Libraries**"
      ],
      "metadata": {
        "id": "vzFyMHzExMan"
      },
      "id": "vzFyMHzExMan"
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n"
      ],
      "metadata": {
        "id": "mXL4b5quxPcW"
      },
      "id": "mXL4b5quxPcW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load and Split Data**"
      ],
      "metadata": {
        "id": "V7GVpALqxS_Y"
      },
      "id": "V7GVpALqxS_Y"
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'df' is your DataFrame and 'y' is the target variable (subscription status)\n",
        "X = data.drop('y', axis=1)  # Features\n",
        "y = data['y']  # Target variable\n",
        "\n",
        "# Splitting data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "qDXE61loxdAV"
      },
      "id": "qDXE61loxdAV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "IeIM1QppSKYM"
      },
      "id": "IeIM1QppSKYM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "id": "8Ma1G-LYTPOc"
      },
      "id": "8Ma1G-LYTPOc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "KUnV7TyPTW_J"
      },
      "id": "KUnV7TyPTW_J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "id": "5P66_EphTZLq"
      },
      "id": "5P66_EphTZLq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# Before SMOTE\n",
        "original_counts = Counter(y_train)\n",
        "\n",
        "# After SMOTE\n",
        "resampled_counts = Counter(y_train_resampled)\n",
        "\n",
        "# Create a DataFrame for visualization\n",
        "df = pd.DataFrame({\n",
        "    'Class': ['No', 'Yes'],\n",
        "    'Original': [original_counts[0], original_counts[1]],\n",
        "    'After SMOTE': [resampled_counts[0], resampled_counts[1]]\n",
        "})\n",
        "\n",
        "# Plot the bar chart\n",
        "df.set_index('Class').plot(kind='bar', figsize=(8, 5), color=['skyblue', 'orange'])\n",
        "plt.title('Class Distribution Before and After SMOTE')\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Class')\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.legend(title='Dataset')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uySc-wdITs-k"
      },
      "id": "uySc-wdITs-k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature Scaling**"
      ],
      "metadata": {
        "id": "mTXGG3X-xkmL"
      },
      "id": "mTXGG3X-xkmL"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the training data and transform the test data\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "8WH-C8Z7xpqK"
      },
      "id": "8WH-C8Z7xpqK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model 1 - Random Forest Classifier**"
      ],
      "metadata": {
        "id": "KpqITR45xsxM"
      },
      "id": "KpqITR45xsxM"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "rf_predictions = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Random Forest Model Accuracy:\", accuracy_score(y_test, rf_predictions))\n",
        "print(\"Random Forest Classification Report:\\n\", classification_report(y_test, rf_predictions))\n"
      ],
      "metadata": {
        "id": "vIK0QJyLxyN9"
      },
      "id": "vIK0QJyLxyN9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model 2 - Neural Network (MLPClassifier)**"
      ],
      "metadata": {
        "id": "m8YFVOfGx1Bd"
      },
      "id": "m8YFVOfGx1Bd"
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize the MLPClassifier (Neural Network)\n",
        "nn_model = MLPClassifier(hidden_layer_sizes=(50,), max_iter=500, random_state=42)\n",
        "\n",
        "# Train the model on the scaled training data\n",
        "nn_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "nn_predictions = nn_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model by calculating accuracy and classification report\n",
        "print(\"Neural Network Model Accuracy:\", accuracy_score(y_test, nn_predictions))\n",
        "print(\"Neural Network Classification Report:\\n\", classification_report(y_test, nn_predictions))\n"
      ],
      "metadata": {
        "id": "Q-efcHkEx8NA"
      },
      "id": "Q-efcHkEx8NA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Optimize the Model**"
      ],
      "metadata": {
        "id": "__7JTigRUskz"
      },
      "id": "__7JTigRUskz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RandomForestClassifier GridSearchCV for hyperparameter optimization**"
      ],
      "metadata": {
        "id": "F5SOhRFV16vU"
      },
      "id": "F5SOhRFV16vU"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize the RandomForestClassifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid for hyperparameter optimization\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],         # Number of trees in the forest\n",
        "    'max_depth': [None, 10, 20, 30],       # Maximum depth of the tree\n",
        "    'min_samples_split': [2, 5, 10],       # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': [1, 2, 4],         # Minimum number of samples required to be at a leaf node\n",
        "    'bootstrap': [True, False]             # Whether bootstrap samples are used when building trees\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid,\n",
        "                           cv=5, scoring='accuracy', verbose=2, n_jobs=-1)\n",
        "\n",
        "# Perform the grid search on the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model and parameters\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Predictions using the best model\n",
        "rf_predictions = best_rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Optimized Random Forest Model Accuracy:\", accuracy_score(y_test, rf_predictions))\n",
        "print(\"Optimized Random Forest Classification Report:\\n\", classification_report(y_test, rf_predictions))\n"
      ],
      "metadata": {
        "id": "S18gTIS30kNd"
      },
      "id": "S18gTIS30kNd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **MLPClassifier GridSearchCV for hyperparameter optimization**"
      ],
      "metadata": {
        "id": "FKbsJIAO2ONK"
      },
      "id": "FKbsJIAO2ONK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize the MLPClassifier\n",
        "nn_model = MLPClassifier(max_iter=500, random_state=42)\n",
        "\n",
        "# Define the parameter grid for hyperparameter optimization\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  # Various layer configurations\n",
        "    'activation': ['relu', 'tanh', 'logistic'],                # Activation functions\n",
        "    'solver': ['adam', 'sgd', 'lbfgs'],                        # Optimization solvers\n",
        "    'alpha': [0.0001, 0.001, 0.01],                            # L2 regularization parameter\n",
        "    'learning_rate': ['constant', 'adaptive'],                 # Learning rate schedule\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=nn_model, param_grid=param_grid,\n",
        "                           cv=5, scoring='accuracy', verbose=2, n_jobs=-1)\n",
        "\n",
        "# Perform the grid search on the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model and parameters\n",
        "best_nn_model = grid_search.best_estimator_\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Make predictions using the best model\n",
        "nn_predictions = best_nn_model.predict(X_test)\n",
        "\n",
        "# Evaluate the optimized model\n",
        "print(\"Optimized Neural Network Model Accuracy:\", accuracy_score(y_test, nn_predictions))\n",
        "print(\"Optimized Neural Network Classification Report:\\n\", classification_report(y_test, nn_predictions))\n"
      ],
      "metadata": {
        "id": "HdWlmZ5p2als"
      },
      "id": "HdWlmZ5p2als",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate the Model after Optimize**"
      ],
      "metadata": {
        "id": "HemZ1ocI4be8"
      },
      "id": "HemZ1ocI4be8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluation for MLPClassifier**"
      ],
      "metadata": {
        "id": "TCkc1pv3mVEH"
      },
      "id": "TCkc1pv3mVEH"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# For MLPClassifier\n",
        "mlp_best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_mlp = mlp_best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the MLP model performance\n",
        "print(\"MLP Model Evaluation:\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_mlp))\n",
        "print(\"Confusion Matrix:\")\n",
        "conf_matrix_mlp = confusion_matrix(y_test, y_pred_mlp)\n",
        "\n",
        "# Plot confusion matrix for MLP\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(conf_matrix_mlp, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
        "plt.title(\"Confusion Matrix - MLPClassifier\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "G8A3qe9lmWP7"
      },
      "id": "G8A3qe9lmWP7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluation for RandomForestClassifier**"
      ],
      "metadata": {
        "id": "Wvql295hmfU4"
      },
      "id": "Wvql295hmfU4"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# For RandomForestClassifier\n",
        "rf_best_model = random_search.best_estimator_\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_rf = rf_best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the RandomForest model performance\n",
        "print(\"\\nRandomForest Model Evaluation:\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "print(\"Confusion Matrix:\")\n",
        "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "\n",
        "# Plot confusion matrix for RandomForest\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(conf_matrix_rf, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
        "plt.title(\"Confusion Matrix - RandomForestClassifier\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QXDoUW0pmj4r"
      },
      "id": "QXDoUW0pmj4r",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "",
      "name": ""
    },
    "language_info": {
      "name": ""
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}